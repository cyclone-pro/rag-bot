"""
Post-interview processing with BATCH OPTIMIZATION
Processes conversation log and generates embeddings efficiently
"""

import logging
import json
from typing import Dict, Any, List
from datetime import datetime, timezone

from app.utils.sentiment_analyzer import analyze_sentiment, get_sentiment_label
from app.utils.keyword_extractor import extract_keywords
from app.services.embedding_service import get_embedding_service
from app.services.milvus_service import get_milvus_service
from app.services.database import get_db_session
from sqlalchemy import text,cast

logger = logging.getLogger(__name__)


def _parse_timestamp(value: Any) -> int:
    if not value:
        return 0
    if isinstance(value, datetime):
        parsed = value
        if parsed.tzinfo is None:
            parsed = parsed.replace(tzinfo=timezone.utc)
        return int(parsed.timestamp())
    if isinstance(value, (int, float)):
        value_int = int(value)
        return int(value_int / 1000) if value_int > 1_000_000_000_000 else value_int
    if isinstance(value, str):
        raw = value.strip()
        if not raw:
            return 0
        if raw.isdigit():
            return _parse_timestamp(int(raw))
        try:
            raw = raw.replace("Z", "+00:00")
            parsed = datetime.fromisoformat(raw)
            if parsed.tzinfo is None:
                parsed = parsed.replace(tzinfo=timezone.utc)
            return int(parsed.timestamp())
        except Exception:
            return 0
    return 0


async def _fetch_interview_metadata(interview_id: str) -> Dict[str, Any]:
    """Fetch job info and scheduled time from Postgres."""
    async with get_db_session() as session:
        result = await session.execute(
            text(
                """
                SELECT job_id, job_title, job_description, scheduled_time, created_at
                FROM interviews
                WHERE interview_id = :interview_id
                LIMIT 1
                """
            ),
            {"interview_id": interview_id},
        )
        row = result.mappings().first()
        return dict(row) if row else {}


async def process_interview_transcript(
    interview_id: str,
    conversation_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Process interview transcript (OPTIMIZED with batch embedding generation)
    
    Performance improvement:
    - Before: 8 answers = 8 separate model calls (~8 seconds)
    - After: 8 answers = 1 batch call (~1 second)
    - Speedup: 8x faster!
    
    Steps:
    1. Extract Q&A pairs from turns
    2. BATCH generate embeddings for all answers (FAST!)
    3. Analyze sentiment for each answer
    4. Extract keywords from each answer
    5. Save embeddings to Milvus
    6. Update conversation_log with milvus_ids
    7. Save to PostgreSQL
    8. Calculate overall metrics
    9. Refresh materialized view
    
    Args:
        interview_id: Interview ID
        conversation_data: Full conversation JSON from ConversationLogger
    
    Returns:
        Processed interview data
    """
    logger.info(f"üîÑ Processing transcript for {interview_id}")
    
    try:
        # Extract Q&A pairs from turns
        qa_pairs = extract_qa_pairs_from_turns(conversation_data["turns"])
        
        if not qa_pairs:
            logger.warning(f"‚ö†Ô∏è  No Q&A pairs found for {interview_id}")
            await save_basic_transcript(interview_id, conversation_data)
            return conversation_data
        
        logger.info(f"üìã Extracted {len(qa_pairs)} Q&A pairs")
        
        # Get services
        embedding_service = get_embedding_service()
        milvus_service = get_milvus_service()

        job_data = conversation_data.get("job", {}) or {}
        db_meta = await _fetch_interview_metadata(interview_id)

        job_id = str(job_data.get("job_id") or db_meta.get("job_id") or "").strip()
        job_title = str(job_data.get("title") or db_meta.get("job_title") or "").strip()
        job_description = str(job_data.get("description") or db_meta.get("job_description") or "").strip()

        scheduled_time = db_meta.get("scheduled_time") or db_meta.get("created_at")
        interview_date = _parse_timestamp(
            scheduled_time or conversation_data.get("metadata", {}).get("start_time")
        )
        
        # ‚úÖ BATCH PROCESSING: Extract all answers first
        answers = [qa["answer"] for qa in qa_pairs]
        
        # ‚úÖ Generate all embeddings in ONE batch (MUCH faster than loop)
        logger.info(f"üß† Generating {len(answers)} embeddings in batch...")
        embeddings = embedding_service.batch_generate_embeddings(
            texts=answers,
            prefix="passage"
        )
        logger.info(f"‚úÖ Generated {len(embeddings)} embeddings")
        
        # Process each Q&A pair with pre-computed embeddings
        processed_qa_pairs = []
        embeddings_data = []
        
        for idx, (qa, embedding) in enumerate(zip(qa_pairs, embeddings), 1):
            # Sentiment analysis
            sentiment = analyze_sentiment(qa["answer"])
            
            # Keyword extraction
            keywords = extract_keywords(qa["answer"])
            
            # Milvus ID
            milvus_id = f"milvus_{interview_id}_{idx}"
            
            # Prepare processed Q&A
            processed_qa = {
                "index": idx,
                "question": qa["question"],
                "answer": qa["answer"],
                "asked_at": qa["asked_at"],
                "answered_at": qa["answered_at"],
                "duration_seconds": qa.get("duration_seconds", 0),
                "sentiment": sentiment["score"],
                "sentiment_label": get_sentiment_label(sentiment["score"]),
                "keywords": keywords,
                "milvus_id": milvus_id
            }
            
            processed_qa_pairs.append(processed_qa)
            
            # Prepare for Milvus
            embeddings_data.append({
                "id": milvus_id,
                "interview_id": interview_id,
                "candidate_id": conversation_data["candidate"]["candidate_id"],
                "job_id": job_id,
                "job_title": job_title,
                "job_description": job_description,
                "question_index": idx,
                "answer_snippet": qa["answer"][:500],
                "interview_date": interview_date,
                "embedding": embedding  # ‚úÖ Already computed in batch
            })
        
        # Save to Milvus
        logger.info(f"üíæ Saving {len(embeddings_data)} embeddings to Milvus...")
        milvus_service.insert_embeddings(embeddings_data)
        
        # Calculate overall metrics
        avg_sentiment = sum(qa["sentiment"] for qa in processed_qa_pairs) / len(processed_qa_pairs)
        
        # Extract all keywords and deduplicate
        all_keywords = [
            kw 
            for qa in processed_qa_pairs 
            for kw in qa["keywords"]["tech_keywords"]
        ]
        unique_keywords = list(set(all_keywords))
        
        # Update conversation_data
        conversation_data["qa_pairs"] = processed_qa_pairs
        conversation_data["metrics"] = {
            "average_sentiment": avg_sentiment,
            "total_questions": len(processed_qa_pairs),
            "keywords_mentioned": unique_keywords
        }
        
        # Save to PostgreSQL
        await save_to_database(interview_id, conversation_data, avg_sentiment, unique_keywords)
        
        # Refresh materialized view
        await refresh_materialized_view()
        
        logger.info(f"‚úÖ Processed transcript for {interview_id}")
        return conversation_data
        
    except Exception as e:
        logger.error(f"‚ùå Error processing transcript: {e}", exc_info=True)
        # Try to save basic transcript anyway
        try:
            await save_basic_transcript(interview_id, conversation_data)
        except:
            pass
        raise


def extract_qa_pairs_from_turns(turns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Extract Q&A pairs from conversation turns
    
    Logic: Agent turn = question, next candidate turn = answer
    """
    qa_pairs = []
    current_question = None
    
    for turn in turns:
        if turn["speaker"] == "agent":
            # New question
            current_question = {
                "question": turn["text"],
                "asked_at": turn["timestamp"]
            }
        elif turn["speaker"] == "candidate" and current_question:
            # Answer to previous question
            try:
                asked_dt = datetime.fromisoformat(current_question["asked_at"])
                answered_dt = datetime.fromisoformat(turn["timestamp"])
                duration = (answered_dt - asked_dt).total_seconds()
            except:
                duration = 0
            
            qa_pairs.append({
                "question": current_question["question"],
                "answer": turn["text"],
                "asked_at": current_question["asked_at"],
                "answered_at": turn["timestamp"],
                "duration_seconds": duration
            })
            current_question = None
    
    return qa_pairs


async def save_to_database(
    interview_id: str,
    conversation_data: Dict[str, Any],
    avg_sentiment: float,
    keywords: List[str]
):
    """Save processed data to PostgreSQL"""
    
    # Get full transcript as text
    turns = conversation_data.get("turns", [])
    full_text_lines = []
    for turn in turns:
        speaker = "Interviewer" if turn["speaker"] == "agent" else "Candidate"
        full_text_lines.append(f"{speaker}: {turn['text']}")
    full_transcript = "\n\n".join(full_text_lines)
    
    # Get duration
    duration = conversation_data.get("metadata", {}).get("duration_seconds", 0)
    
    async with get_db_session() as session:
        # Use all named parameters (no mixing with $1, $2)
        query = text("""
            UPDATE interviews
            SET 
                conversation_log = CAST(:conversation_log AS jsonb),
                full_transcript = :full_transcript,
                interview_status = 'completed',
                sentiment_score = :sentiment_score,
                keyword_matches = CAST(:keyword_matches AS jsonb),
                call_duration_seconds = :duration,
                completed_at = NOW(),
                updated_at = NOW()
            WHERE interview_id = :interview_id
        """)
        
        await session.execute(
            query,
            {
                "interview_id": interview_id,
                "conversation_log": json.dumps(conversation_data),
                "full_transcript": full_transcript,
                "sentiment_score": avg_sentiment,
                "keyword_matches": json.dumps({"keywords": keywords}),
                "duration": duration
            }
        )
        
        logger.info(f"üíæ Saved to PostgreSQL: {interview_id}")


async def save_basic_transcript(
    interview_id: str,
    conversation_data: Dict[str, Any]
):
    """Save basic transcript without processing (fallback)"""
    
    # Get full transcript as text
    turns = conversation_data.get("turns", [])
    full_text_lines = []
    for turn in turns:
        speaker = "Interviewer" if turn["speaker"] == "agent" else "Candidate"
        full_text_lines.append(f"{speaker}: {turn['text']}")
    full_transcript = "\n\n".join(full_text_lines)
    
    # Get duration
    duration = conversation_data.get("metadata", {}).get("duration_seconds", 0)
    
    async with get_db_session() as session:
        # Use all named parameters
        query = text("""
            UPDATE interviews
            SET 
                conversation_log = CAST(:conversation_log AS jsonb),
                full_transcript = :full_transcript,
                interview_status = 'completed',
                call_duration_seconds = :duration,
                completed_at = NOW(),
                updated_at = NOW()
            WHERE interview_id = :interview_id
        """)
        
        await session.execute(
            query,
            {
                "interview_id": interview_id,
                "conversation_log": json.dumps(conversation_data),
                "full_transcript": full_transcript,
                "duration": duration
            }
        )
        
        logger.info(f"üíæ Saved basic transcript to PostgreSQL: {interview_id}")


async def refresh_materialized_view():
    """Refresh the interview_qa_flat materialized view"""
    try:
        async with get_db_session() as session:
            await session.execute(text("REFRESH MATERIALIZED VIEW  interview_qa_flat"))
            logger.info("‚úÖ Refreshed materialized view")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Failed to refresh materialized view: {e}")
        # Non-critical, continue anyway
